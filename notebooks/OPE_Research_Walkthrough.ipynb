{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c9395e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1.Initialization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from src.environment import RecSysEnv\n",
    "from src.policies import get_logging_policy, get_target_policy\n",
    "from src.estimators import calculate_ips, calculate_dr\n",
    "from src.utils import plot_ope_convergence\n",
    "\n",
    "# Initialize our research environment\n",
    "env = RecSysEnv(n_arms=5, context_dim=10)\n",
    "pi_0 = get_logging_policy(n_arms=5, context_dim=10)\n",
    "pi_e = get_target_policy(n_arms=5, context_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40fe38e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Ground Truth: The Oracle\n",
    "def get_ground_truth(env, policy, iterations=10000):\n",
    "    rewards = []\n",
    "    for _ in range(iterations):\n",
    "        ctx = env.get_context()\n",
    "        action, _ = policy.select_action(ctx)\n",
    "        rewards.append(env.get_reward(ctx, action))\n",
    "    return np.mean(rewards)\n",
    "\n",
    "v_true = get_ground_truth(env, pi_e)\n",
    "print(f\"True Value of Target Policy (Ground Truth): {v_true:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac50f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Data Collection (The Logged Dataset)\n",
    "data_logs = []\n",
    "for _ in range(5000):\n",
    "    ctx = env.get_context()\n",
    "    action, prob_0 = pi_0.select_action(ctx)\n",
    "    reward = env.get_reward(ctx, action)\n",
    "    \n",
    "    # Get the probability that the NEW policy would have taken this action\n",
    "    prob_e = pi_e.get_action_probabilities(ctx)[action]\n",
    "    \n",
    "    data_logs.append({\n",
    "        'reward': reward,\n",
    "        'p0': prob_0,\n",
    "        'pe': prob_e\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6aafdb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Running the Estimators\n",
    "# IPS Estimate\n",
    "ips_est = (df['reward'] * (df['pe'] / df['p0'])).mean()\n",
    "\n",
    "print(f\"Ground Truth: {v_true:.4f}\")\n",
    "print(f\"IPS Estimate: {ips_est:.4f}\")\n",
    "print(f\"Estimation Error: {abs(v_true - ips_est):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4b9a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Visualizing the Variance\n",
    "plt.hist(df['pe'] / df['p0'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Distribution of Importance Weights\")\n",
    "plt.xlabel(\"Weight (pi_e / pi_0)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
